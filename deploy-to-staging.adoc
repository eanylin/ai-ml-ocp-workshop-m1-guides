## Deploy The Model To Staging

The model has been tested in the development environment and is now ready to be deployed to the staging environment.

Once we code is pushed to the git repository, the pipeline will run and deploy it to the staging environment. 

To get started, {{ECLIPSE_CHE_URL}}[access the
CodeReady Workspaces instance], and log in using the username and
password you have been assigned
(e.g.Â `{{ USER_ID }}/{{ CHE_USER_PASSWORD }}`):

## Pipeline

A Tekton {{CONSOLE_URL}}/k8s/ns/{{USER_ID}}-stage/tekton.dev\~v1alpha1~Pipeline[pipeline]
has been created for you.

image:pipeline.png[pipeline]

The pipeline consists of the following stages

* Git clone the source code
* Run `train-stage.sh` to train the model
* Build the image using S2I (Source-to-Image)
* Push the image into OpenShift's registry
* Deploy the model into the {{USER_ID}}-stage namespace

### Training script

`train-stage.sh` has been written to be used in the pipeline to call your python script and saves the model into the pipeline workspace. `train-stage.sh` can be found in your {{GIT_URL}}/{{USER_ID}}/datascience-examples/[Gogs] repository. 

[source, sh]
====
#!/bin/bash

pip install -U pip setuptools
pip install -r /workspace/source/workshop/src/train/requirements.txt

echo "Running training"
python /workspace/source/workshop/src/train/lr.py -m /workspace/model -r lr
====

### Building The Image

Once the model has been saved into the pipeline workspace, the building stage will begin to assemble the image by calling `S2I` and `buildah`.

https://developers.redhat.com/blog/2019/02/21/podman-and-buildah-for-docker-users/[Buildah] is a tool that facilitates building Open Container Initiative (OCI) container images.

image::buildah.png[buildah, 700]

S2I follows a well https://docs.openshift.com/container-platform/4.4/builds/build-strategies.html#images-create-s2i-build_build-strategie[defined] repository https://github.com/sclorg/s2i-python-container/tree/master/3.6[layout], which includes the following files:

* `requirements.txt`, for python dependencies
* `.s2i/bin/assemble`, to customize image assembling 
* `.s2i/environment`, custom environment variables
* `app.py`, python script to launch the application

In our workshop, we have overridden `APP_SCRIPT` environment variable to use `app.sh` to launch the model.

The source code is available {{GIT_URL}}/{{USER_ID}}/datascience-examples/src/staging/workshop/src/seldon[here] 

### Deployment

Once the image has been pushed to OpenShift's internal registry, we will deploy the model using Seldon's operator by creating a `SeldonDeployment` resource.

Seldon Core, an open-source framework, makes it easier and faster to deploy your machine learning models and experiments at scale on Kubernetes. Seldon Core extends Kubernetes with its own custom resource `SeldonDeployment` where you can define your runtime inference graph made up of models and other components that Seldon will manage.

A `SeldonDeployment` is a JSON or YAML file that allows you to define your graph of component images and the resources each of those images will need to run (using a Kubernetes PodTemplateSpec). The parts of a SeldonDeployment are shown below:

image::seldon-inf-graph.png[seldon-inf-graph]

The yaml file used in this workshop is available {{GIT_URL}}/{{USER_ID}}/datascience-examples/src/master/workshop/pipeline/seldon-simple-model.yaml[here].

## Push To Staging Branch

Now let's push the code to the staging branch so that the pipeline will run.

[source,sh,role="copypaste"]
====
git checkout -b staging
git push -u -v origin staging
====

image::gogs-staging-branch.png[gogs-staging-branch, 300]

Because Gogs has been configure with a https://{{GIT_URL}}/{{USER_ID}}/datascience-examples/settings/hooks[webhook], a git push will trigger our pipeline.

[WARNING]
====
Do not change the webhook.
====

You can go to OpenShift Console to monitor the {{CONSOLE_URL}}/k8s/ns/{{USER_ID}}-stage/tekton.dev\~v1alpha1~PipelineRun[pipeline run].

Once the model has been deployed, a `Deployment` resource will be created. The pods should be running and in ready state.

You can view them under OpenShift Console -> Workloads -> Deployments. Choose the `{{USER_ID}}-stage` project.

[NOTE]
====
You will notice there is a Seldon service orchestrator pod running. The service orchestrator is a component that is added to your inference graph to:

* Correctly manage the request/response paths described by your inference graph
* Expose Prometheus metrics
* Provide Tracing via Open Tracing
* Add CloudEvent based payload logging

image::seldon-svc-orch.png[seldon-svc-orch]
====

The image would have been pushed into OpenShift {{CONSOLE_URL}}/k8s/ns/{{USER_ID}}-stage/imagestreams[registry] and is tagged with the git revision number. This allows us to provide model provenance by tracking the source code, data version used and the image being used.

## Model Testing

Once the model has been deployed and is running, you now can run some simple test. The test will send sample data to the prediction endpoint. 

[source, sh]
====
/projects/datascience-examples/workshop/bin/stage-test.sh
====

## End-To-End Testing
