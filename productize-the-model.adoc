== Productize The Model

Now the model has been created, we will now begin to productize the model in the development environment. 

We will be using CodeReady Workspaces and `juyptext` to convert
the notebook to python code.

== Logging Into CodeReady Workspaces

You will be using Red Hat CodeReady Workspaces, an online IDE based on
Eclipse Che. Built on the open Eclipse Che project, Red Hat CodeReady
Workspaces uses Kubernetes and containers to provide any member of the
development or IT team with a consistent, secure, and zero-configuration
development environment. The user experience is as fast and familiar as
an integrated development environment (IDE) on their laptop.

CodeReady Workspaces is included in OpenShift® and is available in the
OpenShift Operator Hub. Once deployed, CodeReady Workspaces provides
development teams a faster and more reliable foundation on which to
work, and it gives operations centralized control and peace of mind.

To get started, {{ECLIPSE_CHE_URL}}[access the
CodeReady Workspaces instance], and log in using the username and
password you’ve been assigned
(e.g. `{{ USER_ID }}/{{ CHE_USER_PASSWORD }}`):

image::che-login.png[che-login]

Once you log in, you’ll be placed on your personal dashboard. Click on the name of the pre-created workspace on the left, as shown below (the name will be different depending on your assigned number).

image::codeready-dashboard.png[codeready-dashboard]

Upon entering the workspace, click "Open" at the top right hand corner to access the environment.
image::codeready-user-workspace.png[codeready-user-workspace]

Upon successful logging into CodeReadyWorkspaces, select Dashboard -> enter {{USER_ID}}/{{USER_ID}}-workspace and open the file (the
name will be different depending on your assigned number). 

After a minute or two, you’ll be placed in the workspace:

image::che-workspace.png[che-workspace]

[NOTE]
====
If things get weird or your browser appears, you can simply reload the
browser tab to refresh the view.
====

This IDE is based on Eclipse Che (which is in turn based on MicroSoft VS
Code editor).

You can see icons on the left for navigating between project explorer,
search, version control (e.g. Git), debugging, and other plugins. You’ll
use these during the course of this workshop. Feel free to click on them
and see what they do:

Your git repositories will be cloned to `/projects` automatically for you:

* {{USER_MODEL_REPO_NAME}}: Source repository for the model
* {{USER_DEPLOY_REPO_NAME}}: Kubernetes deployment artifacts

_Changes to files are auto-saved every few seconds_, so you don’t need
to explicitly save changes.

== Converting A Notebook To Python Code

Within your workspace, click on *>_ New Terminal*.

[NOTE]
====
You can open a terminal
window for any of the containers running in your Developer workspace.
For the rest of these labs, anytime you need to run a command in a
terminal, you can use the *>_ New Terminal* command on the right.
====

image::che-terminal.png[che-terminal]

Run the the following commands to convert a notebook into a python code.

[source,bash,role="copypaste"]
----
pip install jupytext
cd /projects/{{USER_MODEL_REPO_NAME}}/notebooks/
jupytext "2 building the first model.ipynb"  -o ../src/train/lr.py
----

To inspect the newly converted file, the user can navifate from the file explorer under "Open File"

image::codeready-file-explorer.png[codeready-file-explorer]

[NOTE]
====
You can ignore the pylint warning

image::che-pylint.png[che-pylint]
====

The python code generated from the notebook has been refactored into a
python class and visualization removed.


== Refactor the New Model

Next, we will then modify the code into a format that the pipeline can
run to train and build the image with the model. The pipeline will call
`train-stage.sh` and expects the model to be written to a folder at
`/workspace/model`. 

We have prepared the refactored model at `/projects/{{USER_MODEL_REPO_NAME}}/src/train/lr.ans.py`. Copy it to `/projects/{{USER_MODEL_REPO_NAME}}/src/train/lr.py`

[source,bash,role="copypaste"]
----
cp /projects/{{USER_MODEL_REPO_NAME}}/src/train/lr.ans.py /projects/{{USER_MODEL_REPO_NAME}}/src/train/lr.py
----

WARNING: Copying the file over would change the contents of the lr.py file, but since this is a prepared file the contents replacing the original lr.py file would be accurate. User can view the new contents of the file via the file explorer as stated previously.

Note that in the lr.py file, there is a section on the version of the data used.
The version of the data used is going to be committed together with source, thus allowing us to have reproducible results
easily with dvc. 

We will also use a bigger training set `creditcard-train.csv` during the training stage in the pipeline.

WARNING: Note that the code directly below this statement is not meant to be run but serves as an illustration for the user to take note of the versioning of the data as well as introduce the use of a bigger training set in the training stage of the pipeline.

[source,python]
----
DATA_VERSION = 'v1.0'

# PIPELINERUN will be set when run from the pipeline
if os.environ.get('PIPELINERUN', None):
    CSV_FILE = 'creditcard-train.csv'       
else:
    CSV_FILE = 'creditcard.csv'
----

== Train And Test The Model

To train the model, run the following script:

[source,bash,role="copypaste"]
----
cd /projects/{{USER_MODEL_REPO_NAME}}
. src/train/config.sh
src/train/train-dev.sh
----

The model would have been written to `/opt/app-root/src/model`. 

As part of the training, the metrics and model will be logged at https://mlflow-{{USER_ID}}-dev.{{ROUTE_SUBDOMAIN}}[MLflow server^]. 

Now start serving the model using REST. A model wrapper has been written to serve the model using Seldon. The model will be loaded and `predict_proba` method will be called. Seldon will wrap the model using Flask and exposes the port `5000`.

WARNING: Note that the code directly below this statement is not meant to be run but serves as an illustration of the model wrapper.

[source,python]
----
class LRModel(Base):
    def __init__(self):
        # Load the model
    def predict(self, X, features_names):
        # Calls the model predict_proba method
----

Serve the model by running `app.sh`.

[source,bash,role="copypaste"]
----
cd /projects/{{USER_MODEL_REPO_NAME}}/src/seldon/
./app.sh
----

[NOTE]
====
You can ignore this popup box because we are not exposing the route.

image::che-exposed-route.png[che-exposed-route]
====

Now let's test the model. Open up a *new* terminal and run the following:

[source,bash,role="copypaste"]
----
/projects/{{USER_MODEL_REPO_NAME}}/bin/dev-test.sh
----

The script will send a fraud and non-fraud requests to the the model. 

== Commit the Code

[source,sh,role="copypaste"]
----
cd /projects/{{USER_MODEL_REPO_NAME}}/src/train
git add *
git commit -a -m 'my lr training code'
git push -v origin master
----

The code has now been pushed to {{GIT_URL}}/{{USER_ID}}/{{USER_MODEL_REPO_NAME}}[your] git
repository on the `master/devel` branch.
